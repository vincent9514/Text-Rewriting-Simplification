{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better aligned Wiki dataset : WikNet\n",
    "http://ssli.ee.washington.edu/tial/projects/simplification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import sys\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('./subword_nmt')\n",
    "from subword_nmt import learn_bpe\n",
    "from subword_nmt import apply_bpe\n",
    "sys.path.append('./nmt/nmt')\n",
    "import textacy\n",
    "\n",
    "# download the princeton wordnet, once run it once\n",
    "# nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-25 02:08:15 URL:http://ssli.ee.washington.edu/tial/projects/simplification/WikNet_word_pairs.txt [32072742/32072742] -> \"./data/WikNet/WikNet_word_pairs.txt\" [1]\n",
      "2018-01-25 02:08:17 URL:http://ssli.ee.washington.edu/tial/projects/simplification/annotations.txt [16000574/16000574] -> \"./data/WikNet/annotations.txt\" [1]\n",
      "2018-01-25 02:08:21 URL:http://ssli.ee.washington.edu/tial/projects/simplification/aligned-good-0.67.txt [41649849/41649849] -> \"./data/WikNet/aligned-good-0.67.txt\" [1]\n",
      "2018-01-25 02:08:24 URL:http://ssli.ee.washington.edu/tial/projects/simplification/aligned-good_partial-0.53.txt [33783304/33783304] -> \"./data/WikNet/aligned-good_partial-0.53.txt\" [1]\n",
      "2018-01-25 02:08:26 URL:http://ssli.ee.washington.edu/tial/projects/simplification/aligned-remaining-0.45.txt [28620520/28620520] -> \"./data/WikNet/aligned-remaining-0.45.txt\" [1]\n"
     ]
    }
   ],
   "source": [
    "#download the scores\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/WikNet_word_pairs.txt -P ./data/WikNet/\n",
    "#download annotations\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/annotations.txt -P ./data/WikNet/\n",
    "#download good alignment\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/aligned-good-0.67.txt -P ./data/WikNet/\n",
    "#download partial alignment\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/aligned-good_partial-0.53.txt -P ./data/WikNet/\n",
    "#downad uncategorized\n",
    "!wget -nv http://ssli.ee.washington.edu/tial/projects/simplification/aligned-remaining-0.45.txt -P ./data/WikNet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiknet_good_file = 'data/WikNet/aligned-good-0.67.txt'\n",
    "wiknet_partial_file = 'data/WikNet/aligned-good_partial-0.53.txt'\n",
    "wiknet_good_prefix = 'data/WikNet/good'\n",
    "wiknet_partial_prefix = 'data/WikNet/partial'\n",
    "\n",
    "wiknet_good_partial_prefix = 'data/WikNet/good_partial'\n",
    "\n",
    "\n",
    "translations = []\n",
    "# generate complex-simple paris in translations\n",
    "def parse_wiknet_file(src_file):\n",
    "    translations = []\n",
    "    scores = []\n",
    "    with open(src_file) as wiknet_file:\n",
    "            for line in wiknet_file:\n",
    "                triplet = line.split(\"\\t\")\n",
    "                translations.append([triplet[0].strip().lower(), triplet[1].strip().lower()])\n",
    "                scores.append(triplet[0].strip())               \n",
    "\n",
    "    return (translations, scores);\n",
    "def split_train_val_test(translations, src_prefix):\n",
    "    np.random.shuffle(translations)\n",
    "    complex_tok_str = [pair[0] for pair in translations]\n",
    "    simple_tok_str = [pair[1] for pair in translations]\n",
    "    \n",
    "    corpus_file = src_prefix+'.all.tok'\n",
    "    \n",
    "    complex_file = src_prefix+'.tok.complex'\n",
    "    simple_file = src_prefix+'.tok.simple'\n",
    "    \n",
    "    complex_train_file = src_prefix+'.train.tok.complex'\n",
    "    simple_train_file = src_prefix+'.train.tok.simple'\n",
    "\n",
    "    complex_val_file = src_prefix+'.val.tok.complex'\n",
    "    simple_val_file = src_prefix+'.val.tok.simple'\n",
    "\n",
    "    complex_test_file = src_prefix+'.test.tok.complex'\n",
    "    simple_test_file = src_prefix+'.test.tok.simple'\n",
    "\n",
    "    val_size = 300\n",
    "    test_size = 300\n",
    "\n",
    "    with open(corpus_file, 'w') as f:\n",
    "        for i in range(len(complex_tok_str)):\n",
    "            f.write(complex_tok_str[i] + '\\n')\n",
    "            f.write(simple_tok_str[i] + '\\n\\n')\n",
    "\n",
    "\n",
    "    with open(complex_file, 'w') as f:\n",
    "        for sent in complex_tok_str:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_file, 'w') as f:\n",
    "        for sent in simple_tok_str:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(complex_train_file, 'w') as f:\n",
    "        for sent in complex_tok_str[:-(val_size+test_size)]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_train_file, 'w') as f:\n",
    "        for sent in simple_tok_str[:-(val_size+test_size)]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(complex_val_file, 'w') as f:\n",
    "        for sent in complex_tok_str[-(val_size+test_size):-test_size]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_val_file, 'w') as f:\n",
    "        for sent in simple_tok_str[-(val_size+test_size):-test_size]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(complex_test_file, 'w') as f:\n",
    "        for sent in complex_tok_str[-test_size:]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open(simple_test_file, 'w') as f:\n",
    "        for sent in simple_tok_str[-test_size:]:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "\n",
    "good_translations, _= parse_wiknet_file(wiknet_good_file)\n",
    "split_train_val_test(good_translations, wiknet_good_prefix)\n",
    "\n",
    "partial_translations, _= parse_wiknet_file(wiknet_partial_file)\n",
    "split_train_val_test(partial_translations, wiknet_partial_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translations = []\n",
    "for pair in good_translations:\n",
    "    if len(pair[1].split())/len(pair[0].split()) < 0.8:\n",
    "        translations.append(pair)\n",
    "for pair in partial_translations:\n",
    "    if len(pair[1].split())/len(pair[0].split()) < 0.8:\n",
    "        translations.append(pair)\n",
    "split_train_val_test(translations, wiknet_good_partial_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn a byte-pair-encoding (bpe) vocabulary using 10,000 merge operations\n",
    "!./subword_nmt/learn_bpe.py -s 10000 < data/WikNet/good_partial.all.tok > data/WikNet/codes.bpe\n",
    "\n",
    "# Apply the vocabulary to the training file\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.all.tok > data/WikNet/good_partial.tok.bpe\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.tok.complex > data/WikNet/good_partial.tok.bpe.complex\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.tok.simple > data/WikNet/good_partial.tok.bpe.simple\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.train.tok.complex > data/WikNet/good_partial.train.tok.bpe.complex\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.train.tok.simple > data/WikNet/good_partial.train.tok.bpe.simple\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.val.tok.complex > data/WikNet/good_partial.val.tok.bpe.complex\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.val.tok.simple > data/WikNet/good_partial.val.tok.bpe.simple\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.test.tok.complex > data/WikNet/good_partial.test.tok.bpe.complex\n",
    "\n",
    "!./subword_nmt/apply_bpe.py -c data/WikNet/codes.bpe < data/WikNet/good_partial.test.tok.simple > data/WikNet/good_partial.test.tok.bpe.simple\n",
    "\n",
    "!cat data/WikNet/good_partial.tok.bpe | ./subword_nmt/get_vocab.py > data/WikNet/good_partial.vocab.tok.bpe\n",
    "!cat data/WikNet/good_partial.tok.bpe.complex | ./subword_nmt/get_vocab.py > data/WikNet/vocab.tok.bpe.complex\n",
    "!cat data/WikNet/good_partial.tok.bpe.simple | ./subword_nmt/get_vocab.py > data/WikNet/vocab.tok.bpe.simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat data/WikNet/good_partial.tok.simple | ./subword_nmt/get_vocab.py > data/WikNet/vocab.tok.simple\n",
    "#!echo \"$(head -n 20000 data/WikNet/vocab.tok.simple)\" >  data/WikNet/vocab.tok.simple\n",
    "!cat data/WikNet/good_partial.tok.complex | ./subword_nmt/get_vocab.py > data/WikNet/vocab.tok.complex\n",
    "#!echo \"$(head -n 20000 data/WikNet/vocab.tok.complex)\" >  data/WikNet/vocab.tok.complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = MosesTokenizer()\n",
    "good_partial_translations = good_translations+partial_translations\n",
    "\n",
    "complex_sentences = [pair[0] for pair in good_partial_translations]\n",
    "simple_sentences = [pair[1] for pair in good_partial_translations]\n",
    "\n",
    "complex_tok_str = [tokenizer.tokenize(sent, return_str = True) for sent in complex_sentences]\n",
    "simple_tok_str = [tokenizer.tokenize(sent, return_str = True) for sent in simple_sentences] \n",
    "\n",
    "complex_tok_words = [word for sent in complex_tok_str for word in sent.split()]\n",
    "simple_tok_words = [word for sent in simple_tok_str for word in sent.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#chars/tokens</th>\n",
       "      <th>#tokens/sentence</th>\n",
       "      <th>#unique tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>8.123335</td>\n",
       "      <td>26.372915</td>\n",
       "      <td>185259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>7.822185</td>\n",
       "      <td>20.048325</td>\n",
       "      <td>143053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         #chars/tokens  #tokens/sentence  #unique tokens\n",
       "name                                                    \n",
       "complex       8.123335         26.372915          185259\n",
       "simple        7.822185         20.048325          143053"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_fdist = nltk.FreqDist(complex_tok_words)\n",
    "simple_fdist = nltk.FreqDist(simple_tok_words)\n",
    "\n",
    "complex_chars = sum([len(word) for word in complex_fdist.keys()])\n",
    "simple_chars = sum([len(word) for word in simple_fdist.keys()])\n",
    "\n",
    "df = pd.DataFrame({'name': ['complex', 'simple'],\n",
    "                   '#tokens/sentence': [complex_fdist.N()/len(complex_sentences), simple_fdist.N()/len(simple_sentences)],\n",
    "                   '#chars/tokens': [complex_chars/complex_fdist.B(), simple_chars/simple_fdist.B()],\n",
    "                   '#unique tokens': [complex_fdist.B(), simple_fdist.B()]})\n",
    "df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before subword segmentation: 200793\n",
      "Number of tokens after subword segmentation: 12301\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(complex_tok_words+simple_tok_words)\n",
    "print(\"Number of tokens before subword segmentation: {}\".format(fdist.B()))\n",
    "\n",
    "text_file = open(r'data/WikNet/good_partial.tok.bpe',\"r\")\n",
    "p = text_file.read()\n",
    "words = word_tokenize(p)\n",
    "fdist = nltk.FreqDist(words)\n",
    "print(\"Number of tokens after subword segmentation: {}\".format(fdist.B()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
